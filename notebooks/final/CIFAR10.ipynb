{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TreResNet on CIFAR\n",
    "* Use ResNet50, Densenet 121\n",
    "* best 4 out of 8 networks \n",
    "* Compare accuracy for 1 to 4 streams\n",
    "* Use standard autgmentation, no test time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rene/code/idc/src\n",
      "1\n",
      "True\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import scipy.io\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50, densenet121\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "PATH = Path('/home/rene/data/cifar')\n",
    "\n",
    "# Add the src directory for functions\n",
    "src_dir = Path.cwd().parent.parent / 'src'\n",
    "print(src_dir)\n",
    "sys.path.append(str(src_dir))\n",
    "\n",
    "# import my functions:\n",
    "from utils import make_cfiar10\n",
    "from models import*\n",
    "from functions import*\n",
    "\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dataset, convert to keras format, make sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL transformed to HTTPS due to an HSTS policy\n",
      "--2018-06-18 19:31:33--  https://pjreddie.com/media/files/cifar.tgz\n",
      "Resolving pjreddie.com (pjreddie.com)... 128.208.3.39\n",
      "Connecting to pjreddie.com (pjreddie.com)|128.208.3.39|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 168584360 (161M) [application/octet-stream]\n",
      "Saving to: ‘/home/rene/data/cifar.tgz’\n",
      "\n",
      "cifar.tgz           100%[===================>] 160.77M  37.7MB/s    in 4.7s    \n",
      "\n",
      "2018-06-18 19:31:39 (34.2 MB/s) - ‘/home/rene/data/cifar.tgz’ saved [168584360/168584360]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ! wget https://pjreddie.com/media/files/cifar.tgz -P /home/rene/data/\n",
    "# ! tar xzf /home/rene/data/cifar.tgz -C /home/rene/data\n",
    "# ! mv /home/rene/data/cifar /home/rene/data/cifar_raw\n",
    "\n",
    "# PATH_IN = Path('/home/rene/data/cifar_raw')\n",
    "# PATH_OUT = PATH\n",
    "# make_cfiar10(PATH_IN, PATH_OUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train 8 Individial ResNet 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/39\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "PATH = Path('/home/rene/data/cifar')\n",
    "save_path = PATH / 'models'\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "num_workers = 6\n",
    "batch_size=80\n",
    "epochs = 40\n",
    "\n",
    "models = {}\n",
    "performance = {}\n",
    "for i in range(8):\n",
    "    models['resnet50_'+str(i)] = resnet50\n",
    "    \n",
    "dataloaders, dataset_sizes = make_batch_gen(str(PATH), batch_size, num_workers, \n",
    "                                            valid_name='valid', size=197)\n",
    "\n",
    "for model_name, model_arch in models.items():\n",
    "    model = model_arch(pretrained=False)\n",
    "    model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "    model = model.cuda()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=int(epochs/3), gamma=0.1)\n",
    "\n",
    "    best_acc, model = train_model(model, criterion, optimizer, exp_lr_scheduler,\n",
    "                                     epochs, dataloaders, dataset_sizes)\n",
    "    torch.save(model.state_dict(), str(save_path / model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Pre-activation ResNet in PyTorch.\n",
    "Reference:\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class PreActBlock(nn.Module):\n",
    "    '''Pre-activation version of the BasicBlock.'''\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "\n",
    "class PreActBottleneck(nn.Module):\n",
    "    '''Pre-activation version of the original Bottleneck module.'''\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(PreActBottleneck, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = self.conv3(F.relu(self.bn3(out)))\n",
    "        out += shortcut\n",
    "        return out\n",
    "\n",
    "\n",
    "class PreActResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(PreActResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def PreActResNet18():\n",
    "    return PreActResNet(PreActBlock, [2,2,2,2])\n",
    "\n",
    "def PreActResNet34():\n",
    "    return PreActResNet(PreActBlock, [3,4,6,3])\n",
    "\n",
    "def PreActResNet50():\n",
    "    return PreActResNet(PreActBottleneck, [3,4,6,3])\n",
    "\n",
    "def PreActResNet101():\n",
    "    return PreActResNet(PreActBottleneck, [3,4,23,3])\n",
    "\n",
    "def PreActResNet152():\n",
    "    return PreActResNet(PreActBottleneck, [3,8,36,3])\n",
    "\n",
    "\n",
    "def test():\n",
    "    net = PreActResNet18()\n",
    "    y = net((torch.randn(1,3,32,32)))\n",
    "    print(y.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0  # best test accuracy\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='/home/rene/data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=80, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='/home/rene/data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=80, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "net = PreActResNet50()\n",
    "device = 'cuda'\n",
    "\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=.005, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=int(epochs/3), gamma=0.1)\n",
    "\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        scheduler.step()\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "#         inputs = Variable(inputs.cuda())\n",
    "#         targets = Variable(targets.cuda())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "    print(100.*correct/total, correct, total)\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    print('test:', 100.*correct/total, correct, total)\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        torch.save(state, str(save_path / 'resnet50_other'))\n",
    "        best_acc = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "18.638 9319 50000\n",
      "test: 18.67 1867 10000\n",
      "Saving..\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'save_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-bb5a715eba3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-48a3cf974079>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         }\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'resnet50_other'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mbest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'save_path' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('/home/rene/data/cifar')\n",
    "save_path = PATH / 'models'\n",
    "save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "num_workers = 6\n",
    "batch_size=80\n",
    "epochs = 20\n",
    "\n",
    "models = {}\n",
    "performance = {}\n",
    "for i in range(8):\n",
    "    models['resnet50_'+str(i)] = resnet50\n",
    "    \n",
    "dataloaders, dataset_sizes = make_batch_gen(str(PATH), batch_size, num_workers, \n",
    "                                            valid_name='valid', size=197)\n",
    "\n",
    "for model_name, model_arch in models.items():\n",
    "    model = model_arch(pretrained=False)\n",
    "    model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "    model = model.cuda()\n",
    "    model = torch.nn.DataParallel(model, device_ids=[0, 1])\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.1)\n",
    "\n",
    "    best_acc, model = train_model_multi(model, criterion, optimizer, exp_lr_scheduler,\n",
    "                                     epochs, dataloaders, dataset_sizes)\n",
    "    torch.save(model.state_dict(), str(save_path / model_name))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Net 121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('/home/rene/data/cifar/sample')\n",
    "num_workers = 6\n",
    "batch_size=32\n",
    "epochs = 15\n",
    "\n",
    "models = {}\n",
    "for i in range(8):\n",
    "    models['densenet121_'+str(i)] = densenet121\n",
    "    \n",
    "dataloaders, dataset_sizes = make_batch_gen(str(PATH), batch_size, num_workers, valid_name='valid', size=224)\n",
    "\n",
    "for model_name, model_arch in models.items():\n",
    "    model = model_arch(pretrained=False)\n",
    "    model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "    model = model.cuda()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "    best_acc, model = train_model(model, criterion, optimizer, exp_lr_scheduler,\n",
    "                                     epochs, dataloaders, dataset_sizes)\n",
    "    torch.save(model.state_dict(), str(PATH /'models' / model_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
